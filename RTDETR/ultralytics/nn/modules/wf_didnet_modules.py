import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
import math

# å°è¯•å¯¼å…¥ Mamba æ ¸å¿ƒç®—å­ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…ä¼šæŠ¥é”™æç¤º
try:
    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn
    from mamba_ssm.modules.mamba_simple import Block as MambaBlock
    IS_MAMBA_AVAILABLE = True
except ImportError:
    IS_MAMBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ° mamba_ssm åº“ã€‚VSSBlock å°†æ— æ³•è¿è¡Œã€‚")
    print("ğŸ‘‰ è¯·è¿è¡Œ: pip install mamba-ssm causal-conv1d>=1.2.0")

# -------------------------------------------------------------------------
# 1. åŸºç¡€å·¥å…·: LayerNorm_NHWC (è§£å†³ Ultralytics NCHW ä¸ Mamba NHWC çš„å†²çª)
# -------------------------------------------------------------------------
class LayerNorm_NHWC(nn.Module):
    """
    ä¸“é—¨ç»™ Mamba ç”¨çš„ LayerNormã€‚
    Ultralytics æ•°æ®æµæ˜¯ [B, C, H, W]ï¼ŒMamba éœ€è¦ [B, H, W, C]ã€‚
    è¿™ä¸ªå±‚è´Ÿè´£åœ¨è¿›å…¥ Mamba å‰è½¬æ¢ç»´åº¦ï¼Œåšå®Œ Norm åä¿æŒ NHWC ç»™ Mamba åƒã€‚
    """
    def __init__(self, normalized_shape, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        # è¾“å…¥ x: [B, C, H, W] -> è½¬ä¸º [B, H, W, C]
        x = x.permute(0, 2, 3, 1)
        return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)

class Permute(nn.Module):
    """ è¾…åŠ©å±‚ï¼šæŠŠ [B, H, W, C] è½¬å› [B, C, H, W] """
    def __init__(self, *args):
        super().__init__()
        self.dims = args
    def forward(self, x):
        return x.permute(*self.dims)

# -------------------------------------------------------------------------
# 2. é¢‘åŸŸæ¨¡å—: DWT (å·²éªŒè¯)
# -------------------------------------------------------------------------
class DWT(nn.Module):
    def __init__(self):
        super().__init__()
        self.requires_grad = False 

    def forward(self, x):
        x01 = x[:, :, 0::2, :] / 2
        x02 = x[:, :, 1::2, :] / 2
        x1 = x01[:, :, :, 0::2]
        x2 = x02[:, :, :, 0::2]
        x3 = x01[:, :, :, 1::2]
        x4 = x02[:, :, :, 1::2]
        x_LL = x1 + x2 + x3 + x4
        x_LH = -x1 - x2 + x3 + x4
        x_HL = -x1 + x2 - x3 + x4
        x_HH = x1 - x2 - x3 + x4
        return torch.cat((x_LL, x_LH, x_HL, x_HH), 1)

# -------------------------------------------------------------------------
# 3. æ ¸å¿ƒç»„ä»¶: SS2D & VSSBlock (Mamba æ ¸å¿ƒ)
# -------------------------------------------------------------------------
class SS2D(nn.Module):
    """
    2D Selective Scan (VMamba æ ¸å¿ƒç®—å­)
    å°† 2D å›¾åƒå±•å¼€ä¸º 4 ä¸ªæ–¹å‘çš„åºåˆ—ï¼Œé€å…¥ Mamba SSM è¿›è¡Œæ‰«æã€‚
    """
    def __init__(self, d_model, d_state=16, d_conv=3, expand=2, dropout=0.):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        
        # 1. è¾“å…¥æŠ•å½± (In_Proj)
        self.in_proj = nn.Linear(d_model, self.d_inner * 2)
        
        # 2. 2D å·ç§¯ (Conv2d) - ç”¨äºå¤„ç†å±€éƒ¨å…³ç³»
        self.conv2d = nn.Conv2d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            groups=self.d_inner,
            bias=True,
            kernel_size=d_conv,
            padding=(d_conv - 1) // 2,
        )
        self.act = nn.SiLU()

        # 3. SSM æ ¸å¿ƒå‚æ•° (A, D, dt, B, C)
        self.x_proj = [
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False)
            for _ in range(4) # 4ä¸ªæ–¹å‘
        ]
        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0))
        del self.x_proj 

        self.dt_projs = [
            self.dt_init(self.dt_rank, self.d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4)
            for _ in range(4)
        ]
        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0))
        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0))
        del self.dt_projs

        # A å’Œ D å‚æ•°
        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True)
        self.Ds = self.D_init(self.d_inner, copies=4, merge=True)

        # 4. è¾“å‡ºæŠ•å½±
        self.out_norm = nn.LayerNorm(self.d_inner)
        self.out_proj = nn.Linear(self.d_inner, d_model)
        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()

    @property
    def dt_rank(self):
        return math.ceil(self.d_model / 16)

    def dt_init(self, dt_rank, d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4):
        m = nn.Linear(dt_rank, d_inner, bias=True)
        dt_init_std = dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(m.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(m.weight, -dt_init_std, dt_init_std)
        
        dt = torch.exp(torch.rand(d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)).clamp(min=dt_init_floor)
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            m.bias.copy_(inv_dt)
        return m

    def A_log_init(self, d_state, d_inner, copies=1, merge=True):
        A = repeat(torch.arange(1, d_state + 1, dtype=torch.float32), "n -> d n", d=d_inner).contiguous()
        A_log = torch.log(A)
        if copies > 1:
            A_log = repeat(A_log, "d n -> r d n", r=copies)
            if merge:
                A_log = A_log.flatten(0, 1)
        A_log = nn.Parameter(A_log)
        A_log._no_weight_decay = True
        return A_log

    def D_init(self, d_inner, copies=1, merge=True):
        D = torch.ones(d_inner)
        if copies > 1:
            D = repeat(D, "n1 -> r n1", r=copies)
            if merge:
                D = D.flatten(0, 1)
        D = nn.Parameter(D)
        D._no_weight_decay = True
        return D

    def forward(self, x):
        # x: [B, H, W, C]
        B, H, W, C = x.shape
        x = self.in_proj(x)
        x, z = x.chunk(2, dim=-1) # (z: gate, x: info)

        # Permute for Conv2d: [B, H, W, C] -> [B, C, H, W]
        x = x.permute(0, 3, 1, 2).contiguous()
        x = self.act(self.conv2d(x)) 
        
        # 3. æ ¸å¿ƒ Mamba æ‰«æåˆ†æ”¯
        if IS_MAMBA_AVAILABLE:
            x_flat = x.flatten(2) # [B, C, L]
            import torch.nn.functional as F
            
            proj_weight = self.x_proj_weight[0] 
            x_dbl = F.linear(x_flat.transpose(1, 2), proj_weight) 
            
            # 1. æ‹†åˆ† dt, B, C
            dt, B_vec, C_vec = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)
            
            # 2. æŠ•å½± dt
            dt = F.linear(dt, self.dt_projs_weight[0]) 
            dt = dt.transpose(1, 2).contiguous()
            
            # 3. å‡†å¤‡ A, B, C, D
            A = -torch.exp(self.A_logs[0:self.d_inner].float()) 
            B_vec = B_vec.transpose(1, 2).contiguous()
            C_vec = C_vec.transpose(1, 2).contiguous()
            D = self.Ds[0:self.d_inner].float()
            
            # 4. è°ƒç”¨ç®—å­
            y = selective_scan_fn(
                x_flat, 
                dt, 
                A, B_vec, C_vec, D, 
                z=None, 
                delta_bias=self.dt_projs_bias[0].float(),
                delta_softplus=True,
                return_last_state=False
            )
            
            y = y.transpose(1, 2).view(B, H, W, -1)
        else:
            y = x.permute(0, 2, 3, 1)

        y = self.out_norm(y)
        y = y * F.silu(z)
        out = self.out_proj(y)
        
        if self.dropout is not None:
            out = self.dropout(out)
        return out

class VSSBlock(nn.Module):
    """
    Ultralytics é€‚é…ç‰ˆ VSS Block
    """
    def __init__(self, hidden_dim, d_state=16):
        super().__init__()
        self.ln_1 = LayerNorm_NHWC(hidden_dim)
        self.self_attention = SS2D(d_model=hidden_dim, d_state=d_state)
        self.ln_2 = nn.LayerNorm(hidden_dim) # FFN å‰çš„ Norm
        
        # FFN (Feed Forward Network)
        self.ffn = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim * 4, 1), # Point-wise
            nn.GELU(),
            nn.Conv2d(hidden_dim * 4, hidden_dim, 1)
        )

    def forward(self, x):
        # x: [B, C, H, W]
        input_x = x
        
        # 1. VSSM éƒ¨åˆ† (éœ€å¤„ç† NHWC)
        x_norm = self.ln_1(x) # å˜æˆ [B, H, W, C]
        x_vss = self.self_attention(x_norm) # è¾“å‡º [B, H, W, C]
        x_vss = x_vss.permute(0, 3, 1, 2) # è½¬å› [B, C, H, W]
        x = input_x + x_vss 

        # 2. FFN éƒ¨åˆ† (Conv å®ç°ï¼Œç›´æ¥åƒ NCHW)
        x_norm2 = x.permute(0, 2, 3, 1) # [B, H, W, C]
        x_norm2 = self.ln_2(x_norm2)
        x_norm2 = x_norm2.permute(0, 3, 1, 2) # [B, C, H, W]
        
        x_ffn = self.ffn(x_norm2)
        x = x + x_ffn
        
        return x

# -------------------------------------------------------------------------
# 4. æ–°å¢: MCAM (YOLO-Extreme) - æ›¿æ¢ SimAM
# -------------------------------------------------------------------------
class MCAM(nn.Module):
    """
    Multi-Dimensional Collaborative Attention Module
    Ref: YOLO-Extreme
    """
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        
        # Channel Branch
        self.fc_channel = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False),
            nn.Sigmoid()
        )
        
        # Height Branch (H-Attention)
        self.conv_h = nn.Conv2d(in_channels, 1, 1)
        self.sigmoid_h = nn.Sigmoid()
        
        # Width Branch (W-Attention)
        self.conv_w = nn.Conv2d(in_channels, 1, 1)
        self.sigmoid_w = nn.Sigmoid()

    def forward(self, x):
        # Channel Attention
        b, c, h, w = x.size()
        y_c = self.fc_channel(self.pool(x)) # [B, C, 1, 1]
        
        # Spatial Attention
        x_h = x.mean(dim=3, keepdim=True) # [B, C, H, 1]
        a_h = self.sigmoid_h(self.conv_h(x_h)) # [B, 1, H, 1]
        
        x_w = x.mean(dim=2, keepdim=True) # [B, C, 1, W]
        a_w = self.sigmoid_w(self.conv_w(x_w)) # [B, 1, 1, W]
        
        return x * y_c * a_h * a_w

# -------------------------------------------------------------------------
# 5. å»é›¾å¤´: HighResMambaDehazeHead
# -------------------------------------------------------------------------
class HighResMambaDehazeHead(nn.Module):
    def __init__(self, in_ch=128, d_state=16, depth=2):
        super().__init__()
        self.proj = nn.Sequential(nn.Conv2d(in_ch, in_ch, 1), nn.BatchNorm2d(in_ch), nn.SiLU())
        
        # ä½¿ç”¨çœŸå®çš„ VSSBlock
        self.mamba_layers = nn.Sequential(*[
            VSSBlock(hidden_dim=in_ch, d_state=d_state) for _ in range(depth)
        ])
        
        # ğŸ”¥ æ³¨æ„ï¼šè¿™é‡Œçš„ mid_conv è¾“å‡ºé€šé“æ•°æ˜¯ 64
        # æ‰€ä»¥ YAML é‡Œçš„ PGM ç¬¬äºŒä¸ªå‚æ•°å¿…é¡»æ˜¯ 64
        self.mid_conv = nn.Sequential(nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU())
        self.t_head = nn.Sequential(nn.Conv2d(64, 1, 1), nn.Sigmoid())
        
        # é‡æ„åˆ†æ”¯
        self.recon = nn.Sequential(
            self._pixel_shuffle_block(64, 32), # H/8
            self._pixel_shuffle_block(32, 16), # H/4
            self._pixel_shuffle_block(16, 16), # H/2
            self._pixel_shuffle_block(16, 8),  # H
            nn.Conv2d(8, 3, 3, 1, 1),
            nn.Sigmoid()
        )

    def _pixel_shuffle_block(self, in_c, out_c):
        return nn.Sequential(
            nn.Conv2d(in_c, out_c * 4, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.ReLU(inplace=True)
        )

#     def forward(self, x_ll, batch=None):
#         # 1. å…¼å®¹æ€§å¤„ç†
#         if isinstance(x_ll, list):
#             x_ll = x_ll[0]
            
#         # 2. æ­£å¸¸çš„å»é›¾é€»è¾‘
#         x = self.proj(x_ll)
#         x = self.mamba_layers(x)
#         feat = self.mid_conv(x)  # è¾“å‡º 64 é€šé“
#         t_map = self.t_head(feat)
        
#         # 3. é‡æ„åˆ†æ”¯
#         recon_img = self.recon(feat)
            
#         # è¿”å›ç»“æœ (Tuple: é€å°„ç‡, æ¢å¤å›¾, ç‰¹å¾)
#         return t_map, recon_img, feat
    def forward(self, x_ll, batch=None):
        if isinstance(x_ll, list):
            x_ll = x_ll[0]

        x = self.proj(x_ll)
        x = self.mamba_layers(x)
        feat = self.mid_conv(x)
        t_map = self.t_head(feat)

    # =========================
    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè®­ç»ƒæ—¶ä¸é‡æ„
    # =========================
#     recon_img = None
#         if not self.training:
#             recon_img = self.recon(feat)

#         return t_map, recon_img, feat
    # é‡æ„åˆ†æ”¯
        # ğŸ”¥ å¿…é¡»æ— æ¡ä»¶ç”Ÿæˆæ¸…æ™°å›¾ï¼Œå› ä¸ºå®ƒæ˜¯åç»­æ£€æµ‹ç½‘ç»œçš„è¾“å…¥ï¼
        recon_img = self.recon(feat) 
            
        # è¿”å›ç»“æœ
        return t_map, recon_img, feat


# =========================================================================
# 6. ç‰©ç†å¼•å¯¼æ¨¡å— (Physical Guidance Module) - å•å‘äº¤äº’
# =========================================================================

class PhysicalGuidanceModule(nn.Module):
    """
    Physical Guidance Module (PGM)
    Dehaze Feature (clean, frozen) -> Detection Feature (trainable)
    """
    def __init__(self, det_dim, dehaze_dim, reduction=16):
        super().__init__()

        # 1. é€šé“å¯¹é½ï¼šdehaze_dim -> det_dim
        # å¦‚æœ YAML å†™ [256, 64]ï¼Œè¿™é‡Œå°±æ˜¯ Conv2d(64, 256)
        self.align_conv = nn.Conv2d(dehaze_dim, det_dim, kernel_size=1)

        # 2. Channel-Selective Fusion (CSFB-like)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        squeeze_dim = max(det_dim // reduction, 16)
        self.fc_squeeze = nn.Sequential(
            nn.Linear(det_dim, squeeze_dim, bias=False),
            nn.ReLU(inplace=True)
        )

        self.fc_alpha = nn.Sequential(
            nn.Linear(squeeze_dim, det_dim, bias=False),
            nn.Sigmoid()
        )
        self.fc_beta = nn.Sequential(
            nn.Linear(squeeze_dim, det_dim, bias=False),
            nn.Sigmoid()
        )

    def forward(self, det_feat, dehaze_feat):
        # =========================
        # âœ… å…³é”®é˜²å¾¡æ€§å¤„ç†
        # =========================
        if dehaze_feat is None:
            return det_feat

        # å¤„ç† list/tuple è¾“å…¥
        if isinstance(dehaze_feat, (list, tuple)):
            dehaze_feat = dehaze_feat[0]

        # 1. é€šé“å¯¹é½
        dehaze_feat = self.align_conv(dehaze_feat)

        # 2. ç©ºé—´å¯¹é½
        if dehaze_feat.shape[2:] != det_feat.shape[2:]:
            dehaze_feat = F.interpolate(
                dehaze_feat,
                size=det_feat.shape[2:],
                mode="bilinear",
                align_corners=False
            )

        # 3. åˆå§‹èåˆ
        x_fused = det_feat + dehaze_feat

        # 4. é€šé“æ³¨æ„åŠ›æƒé‡
        b, c, _, _ = x_fused.size()
        y = self.avg_pool(x_fused).view(b, c)
        y = self.fc_squeeze(y)

        alpha = self.fc_alpha(y).view(b, c, 1, 1)
        beta = self.fc_beta(y).view(b, c, 1, 1)

        # 5. é‡æ ¡å‡† + æ®‹å·®
        out = x_fused + dehaze_feat * alpha + det_feat * beta

        return out

# -------------------------------------------------------------------------
# 7. æ— å‚æ³¨æ„åŠ›: SimAM
# -------------------------------------------------------------------------
class SimAM(nn.Module):
    def __init__(self, e_lambda=1e-4):
        super(SimAM, self).__init__()
        self.activaton = nn.Sigmoid()
        self.e_lambda = e_lambda

    def forward(self, x):
        b, c, h, w = x.size()
        n = h * w - 1
        x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)
        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)) + 0.5
        return x * self.activaton(y)

# -------------------------------------------------------------------------
# 8. å ä½ç¬¦æ¨¡å— (é˜²æ­¢ import æŠ¥é”™)
# -------------------------------------------------------------------------
class SemanticFeedbackModule(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
    def forward(self, x):
        return x

# ğŸ”¥ æ³¨æ„ï¼šè¿™é‡Œåˆ é™¤äº†ä¹‹å‰é‡å¤çš„ class MCAMï¼Œé˜²æ­¢é€»è¾‘è¦†ç›–ï¼