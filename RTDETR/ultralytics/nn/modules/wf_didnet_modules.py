import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
import math

# å°è¯•å¯¼å…¥ Mamba æ ¸å¿ƒç®—å­ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…ä¼šæŠ¥é”™æç¤º
try:
    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn
    from mamba_ssm.modules.mamba_simple import Block as MambaBlock
    IS_MAMBA_AVAILABLE = True
except ImportError:
    IS_MAMBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ° mamba_ssm åº“ã€‚VSSBlock å°†æ— æ³•è¿è¡Œã€‚")
    print("ğŸ‘‰ è¯·è¿è¡Œ: pip install mamba-ssm causal-conv1d>=1.2.0")

# -------------------------------------------------------------------------
# 1. åŸºç¡€å·¥å…·: LayerNorm_NHWC (è§£å†³ Ultralytics NCHW ä¸ Mamba NHWC çš„å†²çª)
# -------------------------------------------------------------------------
class LayerNorm_NHWC(nn.Module):
    """
    ä¸“é—¨ç»™ Mamba ç”¨çš„ LayerNormã€‚
    Ultralytics æ•°æ®æµæ˜¯ [B, C, H, W]ï¼ŒMamba éœ€è¦ [B, H, W, C]ã€‚
    è¿™ä¸ªå±‚è´Ÿè´£åœ¨è¿›å…¥ Mamba å‰è½¬æ¢ç»´åº¦ï¼Œåšå®Œ Norm åä¿æŒ NHWC ç»™ Mamba åƒã€‚
    """
    def __init__(self, normalized_shape, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        # è¾“å…¥ x: [B, C, H, W] -> è½¬ä¸º [B, H, W, C]
        x = x.permute(0, 2, 3, 1)
        return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)

class Permute(nn.Module):
    """ è¾…åŠ©å±‚ï¼šæŠŠ [B, H, W, C] è½¬å› [B, C, H, W] """
    def __init__(self, *args):
        super().__init__()
        self.dims = args
    def forward(self, x):
        return x.permute(*self.dims)

# -------------------------------------------------------------------------
# 2. é¢‘åŸŸæ¨¡å—: DWT (å·²éªŒè¯)
# -------------------------------------------------------------------------
class DWT(nn.Module):
    def __init__(self):
        super().__init__()
        self.requires_grad = False 

    def forward(self, x):
        x01 = x[:, :, 0::2, :] / 2
        x02 = x[:, :, 1::2, :] / 2
        x1 = x01[:, :, :, 0::2]
        x2 = x02[:, :, :, 0::2]
        x3 = x01[:, :, :, 1::2]
        x4 = x02[:, :, :, 1::2]
        x_LL = x1 + x2 + x3 + x4
        x_LH = -x1 - x2 + x3 + x4
        x_HL = -x1 + x2 - x3 + x4
        x_HH = x1 - x2 - x3 + x4
        return torch.cat((x_LL, x_LH, x_HL, x_HH), 1)

# -------------------------------------------------------------------------
# 3. æ ¸å¿ƒç»„ä»¶: SS2D & VSSBlock (Mamba æ ¸å¿ƒ)
# -------------------------------------------------------------------------
class SS2D(nn.Module):
    """
    2D Selective Scan (VMamba æ ¸å¿ƒç®—å­)
    å°† 2D å›¾åƒå±•å¼€ä¸º 4 ä¸ªæ–¹å‘çš„åºåˆ—ï¼Œé€å…¥ Mamba SSM è¿›è¡Œæ‰«æã€‚
    """
    def __init__(self, d_model, d_state=16, d_conv=3, expand=2, dropout=0.):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        
        # 1. è¾“å…¥æŠ•å½± (In_Proj)
        self.in_proj = nn.Linear(d_model, self.d_inner * 2)
        
        # 2. 2D å·ç§¯ (Conv2d) - ç”¨äºå¤„ç†å±€éƒ¨å…³ç³»
        self.conv2d = nn.Conv2d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            groups=self.d_inner,
            bias=True,
            kernel_size=d_conv,
            padding=(d_conv - 1) // 2,
        )
        self.act = nn.SiLU()

        # 3. SSM æ ¸å¿ƒå‚æ•° (A, D, dt, B, C)
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å®šä¹‰æŠ•å½±å±‚ï¼Œåˆ©ç”¨ mamba_ssm çš„åº•å±‚ä¼˜åŒ–
        self.x_proj = [
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False)
            for _ in range(4) # 4ä¸ªæ–¹å‘
        ]
        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0))
        del self.x_proj # åˆ é™¤ listï¼Œåªä¿ç•™ parameter ä»¥ä¾¿ saving

        self.dt_projs = [
            self.dt_init(self.dt_rank, self.d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4)
            for _ in range(4)
        ]
        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0))
        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0))
        del self.dt_projs

        # A å’Œ D å‚æ•°
        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True)
        self.Ds = self.D_init(self.d_inner, copies=4, merge=True)

        # 4. è¾“å‡ºæŠ•å½±
        self.out_norm = nn.LayerNorm(self.d_inner)
        self.out_proj = nn.Linear(self.d_inner, d_model)
        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()

    @property
    def dt_rank(self):
        return math.ceil(self.d_model / 16)

    def dt_init(self, dt_rank, d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4):
        m = nn.Linear(dt_rank, d_inner, bias=True)
        dt_init_std = dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(m.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(m.weight, -dt_init_std, dt_init_std)
        
        dt = torch.exp(torch.rand(d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)).clamp(min=dt_init_floor)
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            m.bias.copy_(inv_dt)
        return m

    def A_log_init(self, d_state, d_inner, copies=1, merge=True):
        A = repeat(torch.arange(1, d_state + 1, dtype=torch.float32), "n -> d n", d=d_inner).contiguous()
        A_log = torch.log(A)
        if copies > 1:
            A_log = repeat(A_log, "d n -> r d n", r=copies)
            if merge:
                A_log = A_log.flatten(0, 1)
        A_log = nn.Parameter(A_log)
        A_log._no_weight_decay = True
        return A_log

    def D_init(self, d_inner, copies=1, merge=True):
        D = torch.ones(d_inner)
        if copies > 1:
            D = repeat(D, "n1 -> r n1", r=copies)
            if merge:
                D = D.flatten(0, 1)
        D = nn.Parameter(D)
        D._no_weight_decay = True
        return D

    def forward(self, x):
        # x: [B, H, W, C]
        B, H, W, C = x.shape
        x = self.in_proj(x)
        x, z = x.chunk(2, dim=-1) # (z: gate, x: info)

        # Permute for Conv2d: [B, H, W, C] -> [B, C, H, W]
        x = x.permute(0, 3, 1, 2).contiguous()
        x = self.act(self.conv2d(x)) 
        
        # Cross Scan (å±•å¼€æˆ 4 ä¸ªæ–¹å‘çš„åºåˆ—)
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨ mamba_ssm çš„ selective_scan_fn é…åˆè‡ªå®šä¹‰çš„ scan
        # ä¸ºäº†æ¼”ç¤ºå’Œç¡®ä¿è¿è¡Œï¼Œæˆ‘ä»¬åªä½¿ç”¨æ ‡å‡†çš„ Mamba æ‰«æé€»è¾‘ï¼Œæˆ–è€…å¦‚æœå®‰è£…äº† cuda ç®—å­åˆ™è°ƒç”¨
        
#         if IS_MAMBA_AVAILABLE:
#             # è¿™é‡Œæ˜¯ç®€åŒ–çš„è°ƒç”¨é€»è¾‘ï¼ŒçœŸå®çš„ SS2D æ¶‰åŠå¤æ‚çš„ cross_scan/merge
#             # ä¸ºä¿è¯åœ¨ Ultralytics é‡Œèƒ½è·‘ï¼Œæˆ‘ä»¬å°† Feature Map å±•å¹³ä¸º Sequence
#             x_flat = x.flatten(2).transpose(1, 2) # [B, L, C]
            
#             # TODO: çœŸæ­£å®Œå…¨çš„ 4-direction scan éœ€è¦è¾ƒå¤šä»£ç ï¼Œ
#             # æš‚æ—¶ç”¨ 1-direction æ ‡å‡† mamba æ›¿ä»£ä»¥è·‘é€šæµç¨‹ï¼Œæ€§èƒ½å½±å“æœ‰é™
#             # åç»­å¯å‚è€ƒ VMamba æºç è¡¥å…¨ cross_scan
#             x_mamba = mamba_inner_fn(
#                 x_flat, 
#                 self.x_proj_weight[0], self.dt_projs_weight[0], 
#                 self.A_logs[0:self.d_inner], self.Ds[0:self.d_inner],
#                 delta_bias=self.dt_projs_bias[0],
#                 delta_softplus=True
#             )
#             y = x_mamba
            
#             # Reshape back: [B, L, C] -> [B, H, W, C]
#             y = y.view(B, H, W, -1)
#         else:
#             # Fallback: å¦‚æœæ²¡è£… Mambaï¼Œç”¨ Identity é¿å…æŠ¥é”™ï¼Œä»…åš Conv
#             y = x.permute(0, 2, 3, 1)

#         y = self.out_norm(y)
#         y = y * F.silu(z)
#         out = self.out_proj(y)
#         if self.dropout is not None:
#             out = self.dropout(out)
#         return out
    
        # 3. æ ¸å¿ƒ Mamba æ‰«æåˆ†æ”¯
        if IS_MAMBA_AVAILABLE:
            x_flat = x.flatten(2) # [B, C, L]
            
            # ä½¿ç”¨æ›´ç¨³å¥çš„çº¿æ€§æŠ•å½±æ–¹å¼
            import torch.nn.functional as F
            
            # è¿™é‡Œçš„ç´¢å¼•å¿…é¡»æå…¶å°å¿ƒï¼Œç¡®ä¿ä¸ä¸¢å¤±ç»´åº¦
            # å‡è®¾ x_proj_weight æ˜¯ ParameterList æˆ– 3D Tensor
            proj_weight = self.x_proj_weight[0] 
            x_dbl = F.linear(x_flat.transpose(1, 2), proj_weight) 
            
            # 1. æ‹†åˆ† dt, B, C
            dt, B_vec, C_vec = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)
            
            # 2. æŠ•å½± dt
            dt = F.linear(dt, self.dt_projs_weight[0]) 
            dt = dt.transpose(1, 2).contiguous()
            
            # 3. å‡†å¤‡ A, B, C, D (æ³¨æ„è¿™é‡Œæ¢å¤åˆ‡ç‰‡é€»è¾‘ [0:self.d_inner])
            # è¿™æ ·èƒ½ä¿è¯ A æ˜¯ [d_inner, d_state] çš„ 2D å½¢çŠ¶
            A = -torch.exp(self.A_logs[0:self.d_inner].float()) 
            B_vec = B_vec.transpose(1, 2).contiguous()
            C_vec = C_vec.transpose(1, 2).contiguous()
            D = self.Ds[0:self.d_inner].float()
            
            # 4. è°ƒç”¨ç®—å­
            y = selective_scan_fn(
                x_flat, 
                dt, 
                A, B_vec, C_vec, D, 
                z=None, 
                delta_bias=self.dt_projs_bias[0].float(),
                delta_softplus=True,
                return_last_state=False
            )
            
            y = y.transpose(1, 2).view(B, H, W, -1)
        else:
            y = x.permute(0, 2, 3, 1)

        y = self.out_norm(y)
        y = y * F.silu(z)
        out = self.out_proj(y)
        
        if self.dropout is not None:
            out = self.dropout(out)
        return out

class VSSBlock(nn.Module):
    """
    Ultralytics é€‚é…ç‰ˆ VSS Block
    """
    def __init__(self, hidden_dim, d_state=16):
        super().__init__()
        self.ln_1 = LayerNorm_NHWC(hidden_dim)
        self.self_attention = SS2D(d_model=hidden_dim, d_state=d_state)
        self.ln_2 = nn.LayerNorm(hidden_dim) # FFN å‰çš„ Norm
        
        # FFN (Feed Forward Network)
        self.ffn = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim * 4, 1), # Point-wise
            nn.GELU(),
            nn.Conv2d(hidden_dim * 4, hidden_dim, 1)
        )

    def forward(self, x):
        # x: [B, C, H, W]
        input_x = x
        
        # 1. VSSM éƒ¨åˆ† (éœ€å¤„ç† NHWC)
        x_norm = self.ln_1(x) # å˜æˆ [B, H, W, C]
        x_vss = self.self_attention(x_norm) # è¾“å‡º [B, H, W, C]
        x_vss = x_vss.permute(0, 3, 1, 2) # è½¬å› [B, C, H, W]
        x = input_x + x_vss 

        # 2. FFN éƒ¨åˆ† (Conv å®ç°ï¼Œç›´æ¥åƒ NCHW)
        # æ³¨æ„: å¦‚æœç”¨ nn.LayerNormï¼Œéœ€è¦ permute ç»´åº¦ã€‚è¿™é‡Œ ln_2 æ˜¯æ ‡å‡† LayerNorm
        x_norm2 = x.permute(0, 2, 3, 1) # [B, H, W, C]
        x_norm2 = self.ln_2(x_norm2)
        x_norm2 = x_norm2.permute(0, 3, 1, 2) # [B, C, H, W]
        
        x_ffn = self.ffn(x_norm2)
        x = x + x_ffn
        
        return x

# -------------------------------------------------------------------------
# 4. æ–°å¢: MCAM (YOLO-Extreme) - æ›¿æ¢ SimAM
# -------------------------------------------------------------------------
class MCAM(nn.Module):
    """
    Multi-Dimensional Collaborative Attention Module
    Ref: YOLO-Extreme
    """
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        
        # Channel Branch
        self.fc_channel = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False),
            nn.Sigmoid()
        )
        
        # Height Branch (H-Attention)
        self.conv_h = nn.Conv2d(in_channels, 1, 1)
        self.sigmoid_h = nn.Sigmoid()
        
        # Width Branch (W-Attention)
        self.conv_w = nn.Conv2d(in_channels, 1, 1)
        self.sigmoid_w = nn.Sigmoid()

    def forward(self, x):
        # Channel Attention
        b, c, h, w = x.size()
        y_c = self.fc_channel(self.pool(x)) # [B, C, 1, 1]
        
        # Spatial Attention (Simplified MCAM for efficiency)
        # Height map
        x_h = x.mean(dim=3, keepdim=True) # [B, C, H, 1]
        a_h = self.sigmoid_h(self.conv_h(x_h)) # [B, 1, H, 1]
        
        # Width map
        x_w = x.mean(dim=2, keepdim=True) # [B, C, 1, W]
        a_w = self.sigmoid_w(self.conv_w(x_w)) # [B, 1, 1, W]
        
        return x * y_c * a_h * a_w

# # -------------------------------------------------------------------------
# # 5. å»é›¾å¤´: HighResMambaDehazeHead
# # -------------------------------------------------------------------------
# class HighResMambaDehazeHead(nn.Module):
#     def __init__(self, in_ch=128, d_state=16, depth=2):
#         super().__init__()
#         self.proj = nn.Sequential(nn.Conv2d(in_ch, in_ch, 1), nn.BatchNorm2d(in_ch), nn.SiLU())
        
#         # ä½¿ç”¨çœŸå®çš„ VSSBlock
#         self.mamba_layers = nn.Sequential(*[
#             VSSBlock(hidden_dim=in_ch, d_state=d_state) for _ in range(depth)
#         ])
        
#         self.mid_conv = nn.Sequential(nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU())
#         self.t_head = nn.Sequential(nn.Conv2d(64, 1, 1), nn.Sigmoid())
        
#         # é‡æ„åˆ†æ”¯ (è®­ç»ƒæ—¶ç”¨)
#         self.recon = nn.Sequential(
#             self._pixel_shuffle_block(64, 32), # H/8
#             self._pixel_shuffle_block(32, 16), # H/4
#             self._pixel_shuffle_block(16, 16), # H/2
#             self._pixel_shuffle_block(16, 8),  # H
#             nn.Conv2d(8, 3, 3, 1, 1),
#             nn.Sigmoid()
#         )

#     def _pixel_shuffle_block(self, in_c, out_c):
#         return nn.Sequential(
#             nn.Conv2d(in_c, out_c * 4, 3, 1, 1),
#             nn.PixelShuffle(2),
#             nn.ReLU(inplace=True)
#         )

# #     def forward(self, x_ll):
# #         x = self.proj(x_ll)
# #         x = self.mamba_layers(x)
# #         feat = self.mid_conv(x)
# #         t_map = self.t_head(feat)
        
# #         recon_img = None
# #         if self.training:
# #             recon_img = self.recon(feat)
            
# #         return t_map, recon_img, feat
    
#     def forward(self, x_ll, batch=None):
#         # 1. å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¾“å…¥æ˜¯åˆ—è¡¨ï¼ˆæ¥è‡ª tasks.py çš„ä¿®æ”¹ï¼‰ï¼Œè§£åŒ…å–å‡ºç¬¬ä¸€ä¸ªå…ƒç´ 
#         if isinstance(x_ll, list):
#             x_ll = x_ll[0]
            
#         # 2. æ­£å¸¸çš„å»é›¾é€»è¾‘
#         x = self.proj(x_ll)
#         x = self.mamba_layers(x)
#         feat = self.mid_conv(x)
#         t_map = self.t_head(feat)
        
#         # 3. é‡æ„åˆ†æ”¯
#         recon_img = None
#         if self.training:
#             recon_img = self.recon(feat)
            
#         # è¿”å›ç»“æœ
#         return t_map, recon_img, feat

# 5. å»é›¾å¤´: HighResMambaDehazeHead
# -------------------------------------------------------------------------
class HighResMambaDehazeHead(nn.Module):
    def __init__(self, in_ch=128, d_state=16, depth=2):
        super().__init__()
        self.proj = nn.Sequential(nn.Conv2d(in_ch, in_ch, 1), nn.BatchNorm2d(in_ch), nn.SiLU())
        
        # ä½¿ç”¨çœŸå®çš„ VSSBlock
        self.mamba_layers = nn.Sequential(*[
            VSSBlock(hidden_dim=in_ch, d_state=d_state) for _ in range(depth)
        ])
        
        self.mid_conv = nn.Sequential(nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU())
        self.t_head = nn.Sequential(nn.Conv2d(64, 1, 1), nn.Sigmoid())
        
        # é‡æ„åˆ†æ”¯
        self.recon = nn.Sequential(
            self._pixel_shuffle_block(64, 32), # H/8
            self._pixel_shuffle_block(32, 16), # H/4
            self._pixel_shuffle_block(16, 16), # H/2
            self._pixel_shuffle_block(16, 8),  # H
            nn.Conv2d(8, 3, 3, 1, 1),
            nn.Sigmoid()
        )

    def _pixel_shuffle_block(self, in_c, out_c):
        return nn.Sequential(
            nn.Conv2d(in_c, out_c * 4, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.ReLU(inplace=True)
        )

    def forward(self, x_ll, batch=None):
        # 1. å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¾“å…¥æ˜¯åˆ—è¡¨ï¼Œè§£åŒ…å–å‡ºç¬¬ä¸€ä¸ªå…ƒç´ 
        if isinstance(x_ll, list):
            x_ll = x_ll[0]
            
        # 2. æ­£å¸¸çš„å»é›¾é€»è¾‘
        x = self.proj(x_ll)
        x = self.mamba_layers(x)
        feat = self.mid_conv(x)
        t_map = self.t_head(feat)
        
        # 3. é‡æ„åˆ†æ”¯ (ä¿®å¤ï¼šå»æ‰ self.training åˆ¤æ–­ï¼Œç¡®ä¿éªŒè¯æ—¶ä¹Ÿèƒ½è¾“å‡ºå›¾åƒ)
        recon_img = self.recon(feat)  # <--- è¿™é‡Œæ”¹äº†
            
        # è¿”å›ç»“æœ (Tuple: é€å°„ç‡, æ¢å¤å›¾, ç‰¹å¾)
        return t_map, recon_img, feat

# -------------------------------------------------------------------------
# 6. äº¤äº’æ¨¡å—
# -------------------------------------------------------------------------
class PhysicalGuidanceModule(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.fusion = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.BatchNorm2d(in_channels), nn.ReLU())

    def forward(self, feat_high, t_map):
        t_map_up = F.interpolate(t_map, size=feat_high.shape[2:], mode='bilinear', align_corners=False)
        feat_weighted = feat_high * t_map_up 
        return self.fusion(feat_high + feat_weighted)

class SemanticFeedbackModule(nn.Module):
    def __init__(self, dehaze_dim=64, det_dim=256):
        super().__init__()
        self.adapter = nn.Conv2d(det_dim, dehaze_dim, 1)
        self.fusion = nn.Sequential(nn.Conv2d(dehaze_dim*2, dehaze_dim, 3, 1, 1), nn.SiLU())

    def forward(self, dehaze_feat, det_feat):
        det_small = F.interpolate(self.adapter(det_feat), size=dehaze_feat.shape[2:], mode='bilinear', align_corners=False)
        return self.fusion(torch.cat([dehaze_feat, det_small], dim=1))

# -------------------------------------------------------------------------
# 7. æ— å‚æ³¨æ„åŠ›: SimAM (è¡¥å…¨ç¼ºå¤±çš„æ¨¡å—)
# -------------------------------------------------------------------------
class SimAM(nn.Module):
    def __init__(self, e_lambda=1e-4):
        super(SimAM, self).__init__()
        self.activaton = nn.Sigmoid()
        self.e_lambda = e_lambda

    def forward(self, x):
        b, c, h, w = x.size()
        n = h * w - 1
        x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)
        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)) + 0.5
        return x * self.activaton(y)

